ETL API Local ‚Äì Projeto de Demonstra√ß√£o
Vis√£o Geral:

Este projeto consiste em um ETL (Extract, Transform, Load) desenvolvido em Python, respons√°vel por recuperar, tratar e carregar dados provenientes de uma API Local, criada para fins avaliativos em um desafio t√©cnico para a vaga de Desenvolvedor Pleno.

O pipeline realiza a extra√ß√£o de tr√™s arquivos JSON, aplica transforma√ß√µes e padroniza√ß√µes necess√°rias e insere os dados em um banco de dados relacional PostgreSQL, deixando-os prontos para consumo por ferramentas de BI, como o Power BI.

Projeto Relacionado:
A API utilizada neste ETL est√° dispon√≠vel no reposit√≥rio abaixo:
	üëâ https://github.com/victormendes-dev/Projeto_KURIER_2026_API
Para que o ETL funcione corretamente, √© necess√°rio iniciar a API localmente por meio do arquivo .bat, mantendo-a ativa durante a execu√ß√£o do processo.

Estrutura da Solu√ß√£o:
A solu√ß√£o foi organizada em m√≥dulos bem definidos, seguindo o fluxo natural de um processo de ETL, separando responsabilidades e facilitando manuten√ß√£o, entendimento e evolu√ß√£o do projeto.

src/
  Diret√≥rio principal do projeto, onde est√° concentrada toda a l√≥gica do ETL.

inicializacao/
- Respons√°vel pelo in√≠cio do processo de ETL.
- Cont√©m os scripts que realizam:
- Requisi√ß√µes √† API Local
- Recupera√ß√£o dos arquivos JSON (leads, clientes, vendas)
- Representa a etapa Extract do ETL.

models/
- Camada respons√°vel pela padroniza√ß√£o e modelagem dos dados.
- Define a estrutura das entidades do projeto por meio de classes.
- Garante consist√™ncia de tipos, ordem dos campos e organiza√ß√£o dos dados antes da carga.
- Exemplo: modelagem da entidade LEADS, encapsulando atributos e m√©todo de convers√£o (to_tuple) para facilitar a inser√ß√£o no banco.
- Essa camada representa a base da etapa Transform, assegurando que os dados sigam um padr√£o √∫nico em todo o pipeline.

guardar_dados/
- Respons√°vel por persistir os dados transformados.
- Converte os arquivos JSON em CSV
- Cria uma camada intermedi√°ria de auditoria e rastreabilidade
- Os CSVs gerados s√£o posteriormente utilizados na carga para o banco de dados.

repositorio/
- Diret√≥rio onde ficam armazenados os arquivos CSV padronizados.
- Representa a √°rea de staging do ETL
- Os arquivos presentes neste diret√≥rio s√£o os que efetivamente ser√£o inseridos no banco de dados.

processamento/
- Camada respons√°vel pela inser√ß√£o dos dados no banco de dados.
- L√™ os arquivos CSV do reposit√≥rio
- Cria a fila de inser√ß√£o
- Direciona os dados para as tabelas corretas, com base no nome do arquivo
- Representa a etapa Load do ETL.

database/
- Respons√°vel por centralizar as informa√ß√µes relacionadas ao banco de dados.
- Scripts de conex√£o
- Configura√ß√µes de acesso (host, usu√°rio, senha, database)
- Fun√ß√µes auxiliares para comunica√ß√£o com o PostgreSQL

variaveis_globais/
- Cont√©m vari√°veis utilizadas em todo o escopo do projeto.
- Constantes
- Par√¢metros globais
- Configura√ß√µes reutilizadas em m√∫ltiplos m√≥dulos

index.py
Arquivo principal respons√°vel por orquestrar a execu√ß√£o do ETL, conectando todas as etapas:
-Inicializa√ß√£o
- Transforma√ß√£o
- Persist√™ncia
- Carga no banco

Banco de Dados:
	Tabelas: projeto_kurier_leads, projeto_kurier_clientes, projeto_kurier_vendas
Os scripts de cria√ß√£o das tabelas e inser√ß√µes encontram-se no diret√≥rio scripts_db.

Fluxo do ETL:
-Inicializa√ß√£o da API Local
- Requisi√ß√£o dos dados via API
- Recupera√ß√£o dos arquivos:
	leads.json
	clientes.json
	vendas.json
- Transforma√ß√£o dos arquivos JSON em CSV (auditoria de dados)
- Leitura dos CSVs padronizados
- Cria√ß√£o de fila de inser√ß√£o
- Inser√ß√£o dos dados no banco PostgreSQL

Principais Decis√µes T√©cnicas:
- Uso de API Local: simula um cen√°rio real de integra√ß√£o entre sistemas.
- Convers√£o JSON ‚Üí CSV: cria uma camada intermedi√°ria para auditoria, rastreabilidade e valida√ß√£o futura dos dados.
- Fila de inser√ß√£o: garante controle do fluxo de carga e organiza√ß√£o da inser√ß√£o por tipo de dado.
- PostgreSQL: banco relacional escolhido pela confiabilidade, compatibilidade com BI e facilidade de integra√ß√£o.
- Roteamento por nome do arquivo: o nome do CSV define automaticamente a tabela de destino, simplificando o processo de carga.

Regras de Neg√≥cio Aplicadas:
- Cada arquivo representa uma entidade distinta:
	Leads
	Clientes
	Vendas
- Os dados s√£o padronizados antes da carga:
	Estrutura de colunas consistente
	Tipos de dados compat√≠veis com o banco
- O nome do arquivo define a tabela de destino no banco de dados.
- A convers√£o para CSV garante uma camada de auditoria caso seja necess√°rio validar ou reprocessar os dados.

Banco de Dados:
- Banco: PostgreSQL
- Database: definida como par√¢metro na fun√ß√£o connect_postgres()
- Scripts SQL:
	Cria√ß√£o das tabelas
	Scripts de inser√ß√£o
    Localizados no diret√≥rio: scripts_db

Execu√ß√£o do Projeto
- Clonar este reposit√≥rio
- Clonar e iniciar o projeto da API Local
- Executar o arquivo .bat para subir a API
- Garantir que o banco PostgreSQL esteja ativo
- Executar o script principal do ETL

Resultado Final:
Ao final do processo, os dados estar√£o:
- Tratados e padronizados
- Armazenados em PostgreSQL
- Prontos para an√°lise e visualiza√ß√£o em ferramentas de BI, como o Power BI